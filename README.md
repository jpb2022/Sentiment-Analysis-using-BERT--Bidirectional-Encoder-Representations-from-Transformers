

---

# ğŸ“Š Sentiment Analysis using BERT (Transformers)

This project demonstrates **Sentiment Analysis** on app review text data using **BERT (Bidirectional Encoder Representations from Transformers)**. We fine-tune the pre-trained BERT model from Hugging Face to classify user reviews into **Positive**, **Neutral**, or **Negative** sentiments.

---

## ğŸ” Problem Statement

Given a set of app reviews and their associated rating scores, the goal is to:

* Preprocess and map numeric ratings to sentiment classes.
* Tokenize the review texts using BERT Tokenizer.
* Fine-tune a BERT-based model for multi-class sentiment classification.
* Evaluate model performance and make predictions on new text inputs.

---

## ğŸ› ï¸ Technologies Used

* Python
* PyTorch
* Transformers (Hugging Face)
* Scikit-learn
* Pandas, NumPy, Seaborn, Matplotlib
* Google Colab (runtime environment)

---

## ğŸ“ Dataset

We use a `reviews.csv` file with the following columns:

* `content`: The review text
* `score`: The star rating (1 to 5)

Other metadata columns are also available but are not directly used in training.

---

## ğŸ“˜ Project Structure

```bash
.
â”œâ”€â”€ reviews.csv                 # Input dataset (example format shown in notebook)
â”œâ”€â”€ best_model.bin             # Saved best model (after training)
â”œâ”€â”€ SentimentAnalysisBERT.ipynb# Jupyter notebook/Colab script
â”œâ”€â”€ README.md                  # Project documentation
```

---

## ğŸ§¾ Step-by-Step Guide

### âœ… Step 1: Install and Import Required Libraries

Install packages:

```bash
!pip install transformers torch seaborn scikit-learn --quiet
```

Import essential libraries for modeling, evaluation, and visualization.

---

### âœ… Step 2: Load Dataset

```python
from google.colab import files
uploaded = files.upload()
df = pd.read_csv('reviews.csv')
```

---

### âœ… Step 3: Preprocessing

* Remove missing values.
* Map ratings to sentiment classes:

  * 1â€“2 â†’ Negative (0)
  * 3   â†’ Neutral (1)
  * 4â€“5 â†’ Positive (2)
* Plot sentiment class distribution.

---

### âœ… Step 4: Tokenization using BERT

* Load `bert-base-cased` tokenizer.
* Visualize token length distribution.
* Set `MAX_LEN = 160` based on token distribution.

---

### âœ… Step 5: Create Dataset and DataLoader

Create a custom `Dataset` class and data loaders:

```python
ReviewDataset(Dataset)
```

* Split data into train, validation, and test sets.
* Batch size: 16

---

### âœ… Step 6: Build BERT-based Sentiment Classifier

Define a model using `BertModel` with:

* Dropout
* Fully connected output layer
* Number of classes: 3

---

### âœ… Step 7: Define Optimizer, Scheduler, and Loss Function

* Optimizer: `AdamW`
* Learning rate: `2e-5`
* Scheduler: `get_linear_schedule_with_warmup`
* Loss function: `CrossEntropyLoss`

---

### âœ… Step 8: Training and Evaluation Functions

Two core functions:

* `train_epoch`: Forward, backward pass, and optimization.
* `eval_model`: Evaluation on validation or test sets.

Use gradient clipping (`max_norm=1.0`) to avoid exploding gradients.

---

### âœ… Step 9: Train the Model

Train for 4 epochs and track accuracy/loss:

```python
torch.save(model.state_dict(), 'best_model.bin')
```

Store the best-performing model based on validation accuracy.

---

### âœ… Step 10: Evaluate the Model on Test Data

Load the saved model:

```python
model.load_state_dict(torch.load('best_model.bin'))
```

Evaluate using:

* Accuracy
* CrossEntropy loss
* Confusion matrix (optional)
* Classification report (optional)

---

### âœ… Step 11: Inference on New Text

Use `predict_sentiment()` to predict sentiment for any new text. Outputs:

* Sentiment class
* Confidence score
* Probability for each class

---

## ğŸ’» Sample Output

```plaintext
Text: I love this product! It's amazing and works perfectly!
Predicted: Positive (Confidence: 0.99)
Probabilities: {'negative': 0.0048, 'neutral': 0.0071, 'positive': 0.9880}
```

---

## ğŸ“ˆ Performance Summary

| Metric         | Value   |
| -------------- | ------- |
| Train Accuracy | \~90.1% |
| Val Accuracy   | \~76.5% |
| Test Accuracy  | \~74.7% |

---

## ğŸ“¦ Requirements

* Python >= 3.6
* PyTorch
* Hugging Face Transformers
* scikit-learn
* seaborn
* pandas, numpy
* matplotlib

---

## ğŸ“š References

* [BERT Paper (Devlin et al.)](https://arxiv.org/abs/1810.04805)
* [Hugging Face Transformers](https://huggingface.co/transformers/)
* [PyTorch Documentation](https://pytorch.org/)

---

## ğŸ§  Future Improvements

* Implement multi-lingual BERT (mBERT) for multi-language reviews.
* Hyperparameter tuning (batch size, learning rate).
* Add text cleaning (stop words, emojis, etc.)
* Use model explainability tools (e.g., SHAP, LIME)

---

## ğŸ¤ Contributing

If youâ€™d like to contribute, feel free to fork the repo and submit a pull request. Suggestions, issues, and improvements are welcome!

---

## ğŸ“¬ Contact

**Author**: \[Your Name]
ğŸ“§ Email: [Jitendraguptaaur@gmail.com](Jitendraguptaaur@gmail.com)
ğŸ”— LinkedIn: [Jitendra Kumar IIT KANPUR](https://www.linkedin.com/in/jitendra-kumar-30a78216a/)

---
